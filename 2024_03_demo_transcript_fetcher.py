# -*- coding: utf-8 -*-
"""2024-03 Demo Transcript Fetcher.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mfveHi_Zu-fvVL0QdQQiEj8b1DObPg9M
"""

import streamlit as st
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import csv

# Set your YouTube API key
YOUTUBE_API_KEY = "YOUTUBE_API_KEY"
youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)


def search_youtube_videos(query, max_results=20, region_code='GB'):
    search_response = youtube.search().list(
        q=query,
        type='video',
        part='id,snippet',
        maxResults=max_results,
        regionCode=region_code
    ).execute()

    videos = []
    for item in search_response.get('items', []):
        video_id = item['id']['videoId']
        title = item['snippet']['title']
        published_at = item['snippet']['publishedAt']
        views = get_video_views(video_id)  # Fetch views here
        videos.append({
            'video_id': video_id,
            'title': title,
            'published_at': published_at,
            'views': views  # Include views in the video information
        })

    return videos


def get_video_views(video_id):
    video_response = youtube.videos().list(
        id=video_id,
        part='statistics'
    ).execute()

    views = video_response['items'][0]['statistics']['viewCount']
    return views

def get_video_details(video_id):
    video_response = youtube.videos().list(
        id=video_id,
        part='snippet,statistics'
    ).execute()
    if video_response['items']:
        item = video_response['items'][0]
        title = item['snippet']['title']
        published_at = item['snippet']['publishedAt']
        return title, published_at
    return None, None

def save_transcripts_to_csv(selected_videos):
    csv_file_path = 'selected_videos_transcripts.csv'
    csv_header = ['video_id', 'video_title', 'date', 'video_text']

    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(csv_header)

        for video in selected_videos:
            video_id = video['video_id']
            title, published_at = get_video_details(video_id)
            try:
                transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
                video_text = ' '.join([entry['text'].replace('\n', ' ') for entry in transcript_list])
                writer.writerow([video_id, title, published_at, video_text])
            except Exception as e:
                st.error(f"Error fetching transcript for video {video_id}: {e}")

    st.success(f"Transcripts saved to {csv_file_path}. These transcripts are now being loaded into the database, please standby.")

# Streamlit UI
st.title("YouTube Video Transcript Fetcher")

query = st.text_input("What YouTube Videos are you interested in?")
if query:
    searched_videos = search_youtube_videos(query)
    if searched_videos:
        # Ensure views are included in the video dictionaries
        options = [f"Views: {video['views']} Date: {video['published_at']} Title: {video['title']}" for video in searched_videos]
        selections = st.multiselect("Select videos to fetch transcripts for:", options)
        if st.button("Fetch Transcripts"):
            selected_indices = [options.index(selection) for selection in selections]
            selected_videos = [searched_videos[index] for index in selected_indices]
            save_transcripts_to_csv(selected_videos)
